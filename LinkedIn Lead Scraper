import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
import time
import random
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import json
import os

class LinkedInLeadScraper:
    """
    A tool to scrape LinkedIn and company websites for lead generation.
    Features:
    - LinkedIn profile data extraction
    - Company website contact information extraction
    - Email pattern recognition
    - Data export to CSV/Excel
    - API integration capability
    """
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.leads_data = []
        self.setup_driver()
        
    def setup_driver(self):
        """Set up the Selenium WebDriver with appropriate options."""
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        self.driver = webdriver.Chrome(options=chrome_options)
        
    def search_linkedin(self, keywords, location=None, industry=None, company_size=None, limit=10):
        """
        Search LinkedIn for profiles matching the given criteria.
        
        Args:
            keywords (str): Search keywords
            location (str, optional): Location filter
            industry (str, optional): Industry filter
            company_size (str, optional): Company size filter
            limit (int, optional): Maximum number of results to return
            
        Returns:
            list: List of LinkedIn profile URLs
        """
        print(f"Searching LinkedIn for: {keywords}")
        
        # This is a simplified version. In a real implementation, 
        # you would need to handle LinkedIn authentication and search properly.
        search_url = f"https://www.linkedin.com/search/results/people/?keywords={keywords.replace(' ', '%20')}"
        if location:
            search_url += f"&location={location.replace(' ', '%20')}"
            
        self.driver.get(search_url)
        time.sleep(2)  # Allow page to load
        
        # In a real implementation, you would need to handle login and pagination
        profile_elements = self.driver.find_elements(By.CSS_SELECTOR, ".search-result__info .search-result__result-link")
        
        profile_urls = []
        for element in profile_elements[:limit]:
            href = element.get_attribute("href")
            if href and "/in/" in href:
                profile_urls.append(href.split("?")[0])  # Remove query parameters
                
        return profile_urls
    
    def extract_linkedin_profile(self, profile_url):
        """
        Extract information from a LinkedIn profile.
        
        Args:
            profile_url (str): LinkedIn profile URL
            
        Returns:
            dict: Profile information
        """
        print(f"Extracting data from: {profile_url}")
        
        self.driver.get(profile_url)
        time.sleep(random.uniform(2, 4))  # Random delay to avoid detection
        
        # In a real implementation, you would need to handle LinkedIn's structure
        try:
            name = self.driver.find_element(By.CSS_SELECTOR, ".pv-top-card--list .text-heading-xlarge").text
        except:
            name = "Unknown"
            
        try:
            title = self.driver.find_element(By.CSS_SELECTOR, ".pv-top-card--list .text-body-medium").text
        except:
            title = "Unknown"
            
        try:
            company = self.driver.find_element(By.CSS_SELECTOR, ".pv-top-card--experience-list-item .pv-entity__secondary-title").text
        except:
            company = "Unknown"
            
        try:
            location = self.driver.find_element(By.CSS_SELECTOR, ".pv-top-card--list-bullet .t-16").text
        except:
            location = "Unknown"
            
        # Find company website if available
        company_website = self.find_company_website(company)
        
        # Extract email using pattern matching
        email = self.extract_email_from_text(self.driver.page_source)
        
        return {
            "name": name,
            "title": title,
            "company": company,
            "location": location,
            "linkedin_url": profile_url,
            "company_website": company_website,
            "email": email
        }
    
    def find_company_website(self, company_name):
        """
        Find company website using search engine.
        
        Args:
            company_name (str): Company name
            
        Returns:
            str: Company website URL
        """
        search_url = f"https://www.google.com/search?q={company_name.replace(' ', '+')}+official+website"
        
        try:
            response = requests.get(search_url, headers=self.headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Extract the first search result
            search_results = soup.select(".yuRUbf a")
            if search_results:
                return search_results[0].get("href")
        except Exception as e:
            print(f"Error finding company website: {e}")
            
        return None
    
    def extract_contact_info_from_website(self, website_url):
        """
        Extract contact information from a company website.
        
        Args:
            website_url (str): Company website URL
            
        Returns:
            dict: Contact information
        """
        if not website_url:
            return {}
            
        contact_info = {
            "emails": [],
            "phone_numbers": [],
            "social_media": {}
        }
        
        try:
            # Get the main page
            response = requests.get(website_url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Extract emails
            emails = self.extract_emails_from_text(response.text)
            contact_info["emails"].extend(emails)
            
            # Extract phone numbers
            phone_numbers = self.extract_phone_numbers(response.text)
            contact_info["phone_numbers"].extend(phone_numbers)
            
            # Find contact page
            contact_links = []
            for link in soup.find_all('a', href=True):
                href = link['href']
                text = link.text.lower()
                if 'contact' in text or 'about' in text:
                    if href.startswith('/'):
                        href = website_url + href
                    elif not href.startswith('http'):
                        href = website_url + '/' + href
                    contact_links.append(href)
            
            # Visit contact pages
            for link in contact_links[:2]:  # Limit to first 2 contact pages
                try:
                    response = requests.get(link, headers=self.headers, timeout=10)
                    contact_emails = self.extract_emails_from_text(response.text)
                    contact_phones = self.extract_phone_numbers(response.text)
                    
                    contact_info["emails"].extend(contact_emails)
                    contact_info["phone_numbers"].extend(contact_phones)
                except:
                    continue
            
            # Remove duplicates
            contact_info["emails"] = list(set(contact_info["emails"]))
            contact_info["phone_numbers"] = list(set(contact_info["phone_numbers"]))
            
            # Extract social media
            social_patterns = {
                "linkedin": r'linkedin\.com/company/([^/"\s]+)',
                "twitter": r'twitter\.com/([^/"\s]+)',
                "facebook": r'facebook\.com/([^/"\s]+)'
            }
            
            for platform, pattern in social_patterns.items():
                matches = re.findall(pattern, response.text)
                if matches:
                    contact_info["social_media"][platform] = matches[0]
            
        except Exception as e:
            print(f"Error extracting contact info: {e}")
            
        return contact_info
    
    def extract_emails_from_text(self, text):
        """Extract email addresses from text."""
        email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
        return re.findall(email_pattern, text)
    
    def extract_email_from_text(self, text):
        """Extract the first email address from text."""
        emails = self.extract_emails_from_text(text)
        return emails[0] if emails else None
    
    def extract_phone_numbers(self, text):
        """Extract phone numbers from text."""
        # This is a simplified pattern - would need to be more robust in production
        phone_pattern = r'(\+\d{1,3}[-.\s]?)?(\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4})'
        return re.findall(phone_pattern, text)
    
    def generate_leads(self, keywords, location=None, industry=None, company_size=None, limit=10):
        """
        Generate leads based on search criteria.
        
        Args:
            keywords (str): Search keywords
            location (str, optional): Location filter
            industry (str, optional): Industry filter
            company_size (str, optional): Company size filter
            limit (int, optional): Maximum number of results
            
        Returns:
            list: List of lead data
        """
        profile_urls = self.search_linkedin(keywords, location, industry, company_size, limit)
        
        leads = []
        for url in profile_urls:
            profile_data = self.extract_linkedin_profile(url)
            
            if profile_data["company_website"]:
                contact_info = self.extract_contact_info_from_website(profile_data["company_website"])
                
                # Merge contact info into profile data
                if contact_info.get("emails") and not profile_data["email"]:
                    profile_data["email"] = contact_info["emails"][0]
                
                profile_data["additional_emails"] = contact_info.get("emails", [])
                profile_data["phone_numbers"] = contact_info.get("phone_numbers", [])
                profile_data["social_media"] = contact_info.get("social_media", {})
            
            leads.append(profile_data)
            
        self.leads_data = leads
        return leads
    
    def export_to_csv(self, filename="leads_data.csv"):
        """Export leads data to CSV."""
        if not self.leads_data:
            print("No data to export.")
            return
            
        df = pd.DataFrame(self.leads_data)
        df.to_csv(filename, index=False)
        print(f"Data exported to {filename}")
        
    def export_to_excel(self, filename="leads_data.xlsx"):
        """Export leads data to Excel."""
        if not self.leads_data:
            print("No data to export.")
            return
            
        df = pd.DataFrame(self.leads_data)
        df.to_excel(filename, index=False)
        print(f"Data exported to {filename}")
        
    def close(self):
        """Close the WebDriver."""
        self.driver.quit()


# Example usage
if __name__ == "__main__":
    scraper = LinkedInLeadScraper()
    
    try:
        # Generate leads
        leads = scraper.generate_leads(
            keywords="CTO startup",
            location="San Francisco",
            limit=5
        )
        
        # Print results
        for lead in leads:
            print(f"Name: {lead['name']}")
            print(f"Title: {lead['title']}")
            print(f"Company: {lead['company']}")
            print(f"Email: {lead['email']}")
            print(f"Website: {lead['company_website']}")
            print("---")
        
        # Export data
        scraper.export_to_csv()
        scraper.export_to_excel()
        
    finally:
        scraper.close()
